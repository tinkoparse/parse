import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd

st.set_page_config(page_title="–ü–∞—Ä—Å–µ—Ä Tinko", layout="wide")
st.title("üß∞ –ü–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ tinko.ru")


def get_sections() -> dict:
    """–ü–∞—Ä—Å–∏—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å https://www.tinko.ru/catalog/"""
    url = "https://www.tinko.ru/catalog/"
    r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(r.text, "html.parser")

    sections = {}
    links = soup.select("div.section-title a.section-title__link")
    for link in links:
        name = link.get_text(strip=True)
        href = link["href"]
        full_url = "https://www.tinko.ru" + href
        sections[name] = full_url

    return sections


def get_max_pages(section_url: str) -> int:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ä–∞–∑–¥–µ–ª–µ"""
    debug = {}
    url = section_url + "?count=96&PAGEN_1=1"
    r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(r.text, "html.parser")

    page_links = soup.select("li.pagination__item > div.pagination__link")
    debug["pagination_blocks_found"] = len(page_links)

    page_numbers = []
    for link in page_links:
        text = link.get_text(strip=True)
        if text.isdigit():
            page_numbers.append(int(text))

    max_page = max(page_numbers) if page_numbers else 1
    debug["max_page"] = max_page

    with st.expander(f"üõ†Ô∏è –û—Ç–ª–∞–¥–∫–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {section_url}", expanded=False):
        for k, v in debug.items():
            st.write(f"**{k}**: {v}")

    return max_page


def parse_section(section_name: str, base_url: str) -> pd.DataFrame:
    data = []
    max_pages = get_max_pages(base_url)
    st.info(f"üîç {section_name}: –Ω–∞–π–¥–µ–Ω–æ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")

    status = st.empty()
    progress_bar = st.progress(0, text=f"{section_name}: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ 1 –∏–∑ {max_pages}")

    for page in range(1, max_pages + 1):
        status.text(f"üîÑ –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞: **{section_name}**, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –∏–∑ {max_pages}")
        progress_bar.progress(page / max_pages, text=f"{section_name}: —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –∏–∑ {max_pages}")

        url = f"{base_url}?count=96&PAGEN_1={page}"
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(r.text, "html.parser")
        products = soup.find_all("div", class_="catalog-product")

        for product in products:
            try:
                name = product.find("p", class_="catalog-product__title").get_text(strip=True)
                code = product.find("span", class_="property-name", string="–ö–æ–¥:")
                code = code.find_next_sibling("span").get_text(strip=True) if code else "‚Äî"

                price_spans = product.select("div.catalog-product__price-block-value span[itemprop='price']")
                retail = float(price_spans[0]["content"]) if len(price_spans) > 0 else None
                wholesale = float(price_spans[1]["content"]) if len(price_spans) > 1 else None

                data.append({
                    "–†–∞–∑–¥–µ–ª": section_name,
                    "–ö–æ–¥": code,
                    "–ù–∞–∑–≤–∞–Ω–∏–µ": name,
                    "–†–æ–∑–Ω–∏—á–Ω–∞—è —Ü–µ–Ω–∞ (—Ä—É–±)": retail,
                    "–û–ø—Ç–æ–≤–∞—è —Ü–µ–Ω–∞ (—Ä—É–±)": wholesale
                })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞: {e}")

    status.text(f"‚úÖ –ì–æ—Ç–æ–≤–æ: {section_name} –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
    progress_bar.empty()
    return pd.DataFrame(data)


# –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–¥–µ–ª—ã —Å —Å–∞–π—Ç–∞
sections = get_sections()
selected_sections = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞:", options=list(sections.keys()))

if st.button("üöÄ –ù–∞—á–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥"):
    if not selected_sections:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ä–∞–∑–¥–µ–ª.")
    else:
        full_data = pd.DataFrame()
        for sec in selected_sections:
            df = parse_section(sec, sections[sec])
            full_data = pd.concat([full_data, df], ignore_index=True)

        st.success("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω! –ù–∏–∂–µ ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
        st.dataframe(full_data)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
        excel_file = "tinko_parsed_data.xlsx"
        full_data.to_excel(excel_file, index=False)
        with open(excel_file, "rb") as f:
            st.download_button(
                "üì• –°–∫–∞—á–∞—Ç—å Excel",
                data=f,
                file_name=excel_file,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
