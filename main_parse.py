import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

st.set_page_config(page_title="–ü–∞—Ä—Å–µ—Ä Tinko", layout="wide")
st.title("üß∞ –ü–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ tinko.ru")

# –ó–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã (–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫—É)
sections = {
    "–°—Ä–µ–¥—Å—Ç–≤–∞ –∏ —Å–∏—Å—Ç–µ–º—ã –æ—Ö—Ä–∞–Ω–Ω–æ-–ø–æ–∂–∞—Ä–Ω–æ–π —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏–∏": "https://www.tinko.ru/catalog/category/1/",
    "–°—Ä–µ–¥—Å—Ç–≤–∞ –∏ —Å–∏—Å—Ç–µ–º—ã –æ—Ö—Ä–∞–Ω–Ω–æ–≥–æ —Ç–µ–ª–µ–≤–∏–¥–µ–Ω–∏—è": "https://www.tinko.ru/catalog/category/265/",
    "–°—Ä–µ–¥—Å—Ç–≤–∞ –∏ —Å–∏—Å—Ç–µ–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–æ–º": "https://www.tinko.ru/catalog/category/114/"
}

selected_sections = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞:", options=list(sections.keys()))

@st.cache_data
def get_max_pages(section_url: str) -> int:
    debug = {}

    url = section_url + "?count=96&PAGEN_1=1"
    r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(r.text, "html.parser")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä–æ–π HTML –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    debug["url"] = url
    debug["status_code"] = r.status_code
    debug["html_snippet"] = soup.prettify()[:2000]

    # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    page_links = soup.select("li.pagination__item > div.pagination__link")
    debug["pagination_blocks_found"] = len(page_links)

    page_numbers = []
    for link in page_links:
        text = link.get_text(strip=True)
        if text.isdigit():
            page_numbers.append(int(text))

    debug["page_numbers"] = page_numbers
    max_page = max(page_numbers) if page_numbers else 1
    debug["max_page"] = max_page

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    with st.expander(f"üõ†Ô∏è –û—Ç–ª–∞–¥–∫–∞: {section_url}", expanded=False):
        for k, v in debug.items():
            st.write(f"**{k}**: {v}")

    return max_page





@st.cache_data
def parse_section(section_name: str, base_url: str) -> pd.DataFrame:
    data = []
    max_pages = get_max_pages(base_url)
    st.info(f"üîç {section_name}: {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü...")

    for page in range(1, max_pages + 1):
        url = f"{base_url}?count=96&PAGEN_1={page}"
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(r.text, "html.parser")
        products = soup.find_all("div", class_="catalog-product")

        for product in products:
            try:
                name = product.find("p", class_="catalog-product__title").get_text(strip=True)
                code = product.find("span", class_="property-name", string="–ö–æ–¥:")
                code = code.find_next_sibling("span").get_text(strip=True) if code else "‚Äî"

                price_spans = product.select("div.catalog-product__price-block-value span[itemprop='price']")
                retail = float(price_spans[0]["content"]) if len(price_spans) > 0 else None
                wholesale = float(price_spans[1]["content"]) if len(price_spans) > 1 else None

                data.append({
                    "–†–∞–∑–¥–µ–ª": section_name,
                    "–ö–æ–¥": code,
                    "–ù–∞–∑–≤–∞–Ω–∏–µ": name,
                    "–†–æ–∑–Ω–∏—á–Ω–∞—è —Ü–µ–Ω–∞ (—Ä—É–±)": retail,
                    "–û–ø—Ç–æ–≤–∞—è —Ü–µ–Ω–∞ (—Ä—É–±)": wholesale
                })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞: {e}")

    return pd.DataFrame(data)

if st.button("üöÄ –ù–∞—á–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥"):
    if not selected_sections:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ä–∞–∑–¥–µ–ª.")
    else:
        full_data = pd.DataFrame()
        for sec in selected_sections:
            df = parse_section(sec, sections[sec])
            full_data = pd.concat([full_data, df], ignore_index=True)

        st.success("‚úÖ –ì–æ—Ç–æ–≤–æ! –ù–∏–∂–µ ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
        st.dataframe(full_data)

        # –°–∫–∞—á–∞—Ç—å Excel
        excel_file = "tinko_parsed_data.xlsx"
        full_data.to_excel(excel_file, index=False)
        with open(excel_file, "rb") as f:
            st.download_button("üì• –°–∫–∞—á–∞—Ç—å Excel", data=f, file_name=excel_file, mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
